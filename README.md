# CI2025_lab2

Repository for lab 2 of the course Computational Intelligence

### Problem description

Given a set of cities and the pairwise distances between them, the objective is to find the shortest possible rout that visits each city exactly once and returns to the starting point.

The goal is to find a permutation of the n cities that minimizes the total tour length.

Various instances of the problem are given, some present non zero values on the diagonal, some have non symmetric matrix and do not respect the triangular inequality, others have negative values.

### Solution representation

Each solution is represented as a permutation of city indices.

This encoding ensures that every city is visited exactly once.

The fitness function is the total tour distance computed using the provided distance matrix.

### Genetic algorithm

**Initialization**

The population is initialized using a greedy heuristic combined with random shuffling:

- Greedy tours are generated by iteratively visiting the nearest unvisited city
- Random tours are added to maintain diversity

This balances exploration and good starting solutions.

**Selection**

Parent selection is performed using tournament selection:

- Random subsets of the population are sampled
- The fittest individual in each subset is selected as a parent

This maintains pressure toward better solutions while preserving diversity.

**Crossover**

Offspring are generated using the Edge Recombination Crossover (ERX):

- The operator preserves important edge information between parents
- It creates offspirng that combine edges from both parents while avoiding invalid permutations

**Mutation**

Two mutation operators are applied:

- Swap mutation: randomly swaps two cities in the tour (encourages exploration by small local perturbations)
- 2-opt mutation: applies a local search heuristic, selects two edges and reverses the segment between them

**Elitism**

A fixed number of elite individuals are directly copied to the next generation to preserve the best solutions found so far.

Before insertion, elites are also refined using the 2-opt mutation to exploit local improvements.

**Early stopping and Adaptation**

An early stopping mechanism halts training if no improvement is observed for a fixed number of generations (patience).

To prevent premature convergence in large problems:

- Parameters tuning depends on the problem size (number of cities)
- For small problems, fewer generations are used
- For large problems, population size and generations scale up automatically

### Improvement of the algorithm on problem_g_1000

Here I show an example of improvement of the genetic algoritm, the first row shows the results of a naive implementation (the genetic_algorithm() function in the code). In the second row we can see how small improvements like 2-opt mutation and adaptation to problem size lead to better results with fewer generations, however the main improving factor here is the greedy initialization which as can be seen in more complex problems often remains as the best solution found with the algorithm incapable of improving it. Results for all the instances of the problem can be found in the pdf file.

Best distance: 129_550.61144585899

<p align="center">
  <img src="image\README\1762180679599.png" width="45%" />
  <img src="image\README\1762180700723.png" width="53%" />
</p>

Best distance: 14_090.812364985677

<p align="center">
  <img src="image\README\1762180603144.png" width="45%" />
  <img src="image\README\1762180654150.png" width="53%" />
</p>
